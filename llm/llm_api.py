from fastapi import FastAPI
from pydantic import BaseModel
from llama_cpp import Llama
from starlette.responses import JSONResponse
import os
import time
import boto3
from botocore.exceptions import ClientError

# --- í™˜ê²½ ì„¤ì • (ì—…ë°ì´íŠ¸ë¨) ---
MODEL_FILE_NAME = "gemma-3n-E4B-it-Q4_K_M.gguf" # ğŸ”‘ ìˆ˜ì •ëœ ëª¨ë¸ íŒŒì¼ ì´ë¦„
S3_BUCKET_NAME = os.environ.get("S3_BUCKET_NAME")
S3_MODEL_KEY = os.environ.get("S3_MODEL_KEY", MODEL_FILE_NAME) 
MODEL_LOCAL_PATH = f"/tmp/{MODEL_FILE_NAME}" 

# --- ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---
def download_model_from_s3(bucket_name, key, local_path):
    """S3ì—ì„œ GGUF ëª¨ë¸ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
    if not bucket_name or not key:
        print("ERROR: S3 í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
        
    print(f"S3ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘: s3://{bucket_name}/{key}")
    try:
        s3 = boto3.client('s3')
        os.makedirs(os.path.dirname(local_path) or '.', exist_ok=True)
        s3.download_file(bucket_name, key, local_path)
        print("ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ.")
        return True
    except ClientError as e:
        if e.response['Error']['Code'] == "404":
            print(f"ERROR: S3 ê°ì²´ {key}ë¥¼ ë²„í‚· {bucket_name}ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print(f"ERROR: S3 ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False
    except Exception as e:
        print(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨! ì˜¤ë¥˜: {e}")
        return False

# --- ëª¨ë¸ ì´ˆê¸°í™” ë° FastAPI ì—”ë“œí¬ì¸íŠ¸ ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€ ---
llm = None
if download_model_from_s3(S3_BUCKET_NAME, S3_MODEL_KEY, MODEL_LOCAL_PATH):
    try:
        llm = Llama(
            model_path=MODEL_LOCAL_PATH,
            n_gpu_layers=0,  
            n_ctx=4096,      
            verbose=False    
        )
        print("LLM Model Loaded Successfully!")
    except Exception as e:
        print(f"ERROR: Failed to load LLM model from {MODEL_LOCAL_PATH}. Error: {e}")
        llm = None

app = FastAPI(
    title="Gemma 3N Financial Chat API",
    description="AWS Fargateì—ì„œ S3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” LLM ì¶”ë¡  APIì…ë‹ˆë‹¤.",
)

class ChatRequest(BaseModel):
    prompt: str

class ChatResponse(BaseModel):
    response: str
    time_ms: int

@app.get("/health")
def health_check():
    if llm is None:
        return JSONResponse(status_code=503, content={"status": "DOWN", "message": "LLM not loaded or failed to download"})
    return {"status": "UP", "model_path": MODEL_LOCAL_PATH}

@app.post("/llm/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    if llm is None:
        return JSONResponse(status_code=503, content={"error": "LLM ì„œë²„ ì¤€ë¹„ ì•ˆ ë¨"},)

    start_time = time.time()
    
    prompt_template = f"""
    ë‹¹ì‹ ì€ gemma-3n-E4B-it ê¸°ë°˜ì˜ ì „ë¬¸ ê¸ˆìœµ ë¶„ì„ê°€ ì±—ë´‡ì…ë‹ˆë‹¤.
    ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ë©°, í˜„ì¬ ê¸ˆìœµ ì‹œì¥ ë° ë°ì´í„°ì™€ ê´€ë ¨í•˜ì—¬ ì „ë¬¸ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

    User: {request.prompt}
    Assistant:
    """

    try:
        output = llm(
            prompt_template, 
            max_tokens=1024, 
            stop=["\nUser:", "User:"], 
            echo=False,
            temperature=0.7 
        )
        
        response_text = output['choices'][0]['text'].strip()
        end_time = time.time()
        
        return ChatResponse(
            response=response_text,
            time_ms=int((end_time - start_time) * 1000)
        )
    
    except Exception as e:
        print(f"LLM ì¶”ë¡  ì˜¤ë¥˜: {e}")
        return JSONResponse(
            status_code=500, 
            content={"error": f"LLM ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}
        )